{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IEEE explict.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMdf4CnCETD/lUe9uqmIFBM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"o5VkVmt85POK"},"source":["# Get dataset from another Gdrive account"]},{"cell_type":"code","metadata":{"id":"kg-5ffji5StI"},"source":["def folder_download(folder_id):\n","  # authenticate\n","  from google.colab import auth\n","  auth.authenticate_user()\n","  # get folder_name\n","  from googleapiclient.discovery import build\n","  service = build('drive', 'v3')\n","  folder_name = service.files().get(fileId=folder_id).execute()['name']\n","  # import library and download\n","  !wget -qnc https://github.com/segnolin/google-drive-folder-downloader/raw/master/download.py\n","  from download import download_folder\n","  download_folder(service, folder_id, './', folder_name)\n","  return folder_name\n","\n","# Provide the folder relevant url part \n","folder_download('1Kmlx7fGMGWo76VacEc4PDPDsVkEMe1Zf') # COMPLETE DATASET\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8cJ8G4QI5VZj"},"source":["# Mount my Gdrive\n","\n"]},{"cell_type":"code","metadata":{"id":"PgwTuybf5aPs"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","import sys\n","sys.path.insert(0,'/content/drive/MyDrive/Colab Notebooks/IEEE3D/')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OKl_tDPN5cTE"},"source":["# Install reuirements\n"]},{"cell_type":"code","metadata":{"id":"3wQiO-o15g9a"},"source":["!pip install -r 'drive/MyDrive/Colab Notebooks/IEEE3D/requirements.txt'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gZlWYNTZ5y8R"},"source":["'''\n","Train our baseline model for the task2 of the L3DAS21 challenge.\n","This script saves the best model checkpoint, as well as a dict containing\n","the results (loss and history). To evaluate the performance of the trained model\n","according to the challenge metrics, please use evaluate_baseline_task2.py.\n","Command line arguments define the model parameters, the dataset to use and\n","where to save the obtained results.\n","'''\n"]},{"cell_type":"markdown","metadata":{"id":"lLnRJb4I5hz-"},"source":["# Import libs\n"]},{"cell_type":"code","metadata":{"id":"1LykG9p65u6G"},"source":["import sys, os\n","import time\n","import json\n","import pickle\n","import argparse\n","from tqdm import tqdm\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","import torch.utils.data as utils\n","from SELDNet import Seldnet, Seldnet_augmented\n","from utility_functions import load_model, save_model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QBvB9Q-u9Xd3"},"source":["# Define \"args\""]},{"cell_type":"code","metadata":{"id":"7WP-L3Xf9a3P"},"source":["results_path = 'RESULTS/task2' #'Folder to write results dicts into')\n","checkpoint_dir = 'RESULTS/task2' #'Folder to write checkpoints into')\n","    \n","training_predictors_path='processed/task2_predictors_train.pkl'\n","training_target_path='processed/task2_target_train.pkl'\n","validation_predictors_path', type=str, default='processed/task2_predictors_validation.pkl')\n","validation_target_path', type=str, default='processed/task2_target_validation.pkl')\n","test_predictors_path', type=str, default='processed/task2_predictors_test.pkl')\n","test_target_path', type=str, default='processed/task2_target_test.pkl')\n","\n","gpu_id=0\n","use_cuda='True' #str\n","early_stopping='True' # str\n","fixed_seed='False' # str\n","load_model=None #'Reload a previously trained model (whole task model)')\n","lr=0.00001\n","batch_size=20 #\"Batch size\")\n","sr=32000   #\"Sampling rate\")\n","patience=3 #\"Patience for early stopping on validation set\")\n","loss=\"L2\" #\"L1 or L2\")\n","\n","#model parameters\n","architecture='seldnet' #\"can be seldnet or seldnet_augmented\")\n","time_dim=600\n","freq_dim=256\n","input_channels=4\n","output_classes=14\n","pool_size=[[8,2],[8,2],[2,2]]\n","pool_time=False\n","n_cnn_filters=64\n","rnn_size=128\n","n_rnn=1\n","fc_size=128\n","dropout_perc=0.0\n","verbose=True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q8iw4iok7AdN"},"source":["## Running environment "]},{"cell_type":"code","metadata":{"id":"XyMNlDvH7Ibo"},"source":["device = 'cuda:0'\n","\n","fixed_seed = False\n","if fixed_seed:\n","    seed = 1\n","    np.random.seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9bJPVwau5wKh"},"source":["# Evaluate network (validation)"]},{"cell_type":"code","metadata":{"id":"Fifmx3nu6SOc"},"source":["def evaluate(model, device, criterion, dataloader):\n","    #compute loss without backprop\n","    model.eval()\n","    test_loss = 0.\n","    with tqdm(total=len(dataloader) // batch_size) as pbar, torch.no_grad():\n","        for example_num, (x, target) in enumerate(dataloader):\n","            target = target.to(device)\n","            x = x.to(device)\n","            outputs_sed, outputs_doa = model(x)\n","            loss = criterion(outputs_doa,  target[:,:,:126])\n","            test_loss += (1. / float(example_num + 1)) * (loss - test_loss)\n","            pbar.set_description(\"Current loss: {:.4f}\".format(test_loss))\n","            pbar.update(1)\n","    return test_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2BBjXMpx6hcD"},"source":["# Load Dataset\n"]},{"cell_type":"code","metadata":{"id":"K_mK7Ghb7o4b"},"source":["\n","print ('\\nLoading dataset')\n","\n","with open(training_predictors_path, 'rb') as f:\n","    training_predictors = pickle.load(f)\n","with open(training_target_path, 'rb') as f:\n","    training_target = pickle.load(f)\n","with open(validation_predictors_path, 'rb') as f:\n","    validation_predictors = pickle.load(f)\n","with open(validation_target_path, 'rb') as f:\n","    validation_target = pickle.load(f)\n","with open(test_predictors_path, 'rb') as f:\n","    test_predictors = pickle.load(f)\n","with open(test_target_path, 'rb') as f:\n","    test_target = pickle.load(f)\n","\n","training_predictors = np.array(training_predictors)\n","training_target = np.array(training_target)\n","validation_predictors = np.array(validation_predictors)\n","validation_target = np.array(validation_target)\n","test_predictors = np.array(test_predictors)\n","test_target = np.array(test_target)\n","\n","print ('\\nShapes:')\n","print ('Training predictors: ', training_predictors.shape)\n","print ('Validation predictors: ', validation_predictors.shape)\n","print ('Test predictors: ', test_predictors.shape)\n","\n","#convert to tensor\n","training_predictors = torch.tensor(training_predictors).float()\n","validation_predictors = torch.tensor(validation_predictors).float()\n","test_predictors = torch.tensor(test_predictors).float()\n","training_target = torch.tensor(training_target).float()\n","validation_target = torch.tensor(validation_target).float()\n","test_target = torch.tensor(test_target).float()\n","#build dataset from tensors\n","tr_dataset = utils.TensorDataset(training_predictors, training_target)\n","val_dataset = utils.TensorDataset(validation_predictors, validation_target)\n","test_dataset = utils.TensorDataset(test_predictors, test_target)\n","#build data loader from dataset\n","tr_data = utils.DataLoader(tr_dataset, batch_size, shuffle=True, pin_memory=True)\n","val_data = utils.DataLoader(val_dataset, batch_size, shuffle=False, pin_memory=True)\n","test_data = utils.DataLoader(test_dataset, batch_size, shuffle=False, pin_memory=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iFmEWW4J7xJR"},"source":["# Load MODEL\n"]},{"cell_type":"code","metadata":{"id":"tbfmXSMr70Ck"},"source":["architecture = 'seldnet' # WANDB (test)\n","\n","if architecture == 'seldnet':\n","    model = Seldnet(time_dim=time_dim, freq_dim=freq_dim,\n","                              input_channels=input_channels, output_classes=output_classes,\n","                              pool_size=pool_size, pool_time=pool_time,\n","                              n_cnn_filters=n_cnn_filters, rnn_size=rnn_size, \n","                              n_rnn=n_rnn,fc_size=fc_size, dropout_perc=dropout_perc, \n","                              verbose=verbose)\n","elif architecture == 'seldnet_augmented':\n","    model = Seldnet_augmented()\n","\n","\n","print(\"Moving model to gpu\")\n","model = model.to(device)\n","\n","#compute number of parameters\n","model_params = sum([np.prod(p.size()) for p in model.parameters()])\n","print ('Total paramters: ' + str(model_params))\n","\n","#set up the loss function\n","if loss == \"L1\":\n","    criterion = nn.L1Loss()\n","elif loss == \"L2\":\n","    criterion = nn.MSELoss()\n","else:\n","    raise NotImplementedError(\"Couldn't find this loss!\")\n","\n","#set up optimizer\n","optimizer = Adam(params=model.parameters(), lr=lr)\n","\n","#set up training state dict that will also be saved into checkpoints\n","state = {\"step\" : 0,\n","          \"worse_epochs\" : 0,\n","          \"epochs\" : 0,\n","          \"best_loss\" : np.Inf}\n","\n","#load model checkpoint if desired\n","if load_model is not None:\n","    print(\"Continuing training full model from checkpoint \" + str(load_model))\n","    state = load_model(model, optimizer, load_model, use_cuda)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LAjmxA8f80kG"},"source":["# Train model\n"]},{"cell_type":"code","metadata":{"id":"ZtMlwD9Y83Uv"},"source":["print('TRAINING START')\n","    train_loss_hist = []\n","    val_loss_hist = []\n","    while state[\"worse_epochs\"] < patience:\n","        print(\"Training one epoch from iteration \" + str(state[\"step\"]))\n","        avg_time = 0.\n","        model.train()\n","        train_loss = 0.\n","        with tqdm(total=len(tr_dataset) // batch_size) as pbar:\n","            for example_num, (x, target) in enumerate(tr_data):\n","                target = target.to(device)\n","                x = x.to(device)\n","                t = time.time()\n","                # Compute loss for each instrument/model\n","                optimizer.zero_grad()\n","                outputs_sed, outputs_doa = model(x)\n","\n","                f_loss = criterion(outputs_doa, target[:,:,:126]) \n","\n","\n","                f_loss.backward()\n","\n","                train_loss += (1. / float(example_num + 1)) * (f_loss - train_loss)\n","                optimizer.step()\n","                state[\"step\"] += 1\n","                t = time.time() - t\n","                avg_time += (1. / float(example_num + 1)) * (t - avg_time)\n","\n","                pbar.update(1)\n","\n","            #PASS VALIDATION DATA\n","            val_loss = evaluate(model, device, criterion, val_data)\n","            print(\"VALIDATION FINISHED: LOSS: \" + str(val_loss))\n","\n","            # EARLY STOPPING CHECK\n","            checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint\")\n","\n","            if val_loss >= state[\"best_loss\"]:\n","                state[\"worse_epochs\"] += 1\n","            else:\n","                print(\"MODEL IMPROVED ON VALIDATION SET!\")\n","                state[\"worse_epochs\"] = 0\n","                state[\"best_loss\"] = val_loss\n","                state[\"best_checkpoint\"] = checkpoint_path\n","\n","                # CHECKPOINT\n","                print(\"Saving model...\")\n","                save_model(model, optimizer, state, checkpoint_path)\n","\n","            state[\"epochs\"] += 1\n","            #state[\"worse_epochs\"] = 200\n","            train_loss_hist.append(train_loss.cpu().detach().numpy())\n","            val_loss_hist.append(val_loss.cpu().detach().numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x8TBAJB289iH"},"source":["# Load best model and compute loss for all sets\n"]},{"cell_type":"code","metadata":{"id":"rapw0WA-9C1T"},"source":["print(\"TESTING\")\n","# Load best model based on validation loss\n","state = load_model(model, None, state[\"best_checkpoint\"], use_cuda)\n","#compute loss on all set_output_size\n","train_loss = evaluate(model, device, criterion, tr_data)\n","val_loss = evaluate(model, device, criterion, val_data)\n","test_loss = evaluate(model, device, criterion, test_data)\n","\n","#PRINT AND SAVE RESULTS\n","results = {'train_loss': train_loss.cpu().detach().numpy(),\n","            'val_loss': val_loss.cpu().detach().numpy(),\n","            'test_loss': test_loss.cpu().detach().numpy(),\n","            'train_loss_hist': train_loss_hist,\n","            'val_loss_hist': val_loss_hist}\n","\n","print ('RESULTS')\n","for i in results:\n","    if 'hist' not in i:\n","        print (i, results[i])\n","out_path = os.path.join(results_path, 'results_dict.json')\n","np.save(out_path, results)"],"execution_count":null,"outputs":[]}]}